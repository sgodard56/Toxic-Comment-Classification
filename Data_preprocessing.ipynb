{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my submission to the Toxic Comment Classification Challenge (https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### control the use of the gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read data as dataframes with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_table('data/train.csv',sep=',',header=0)\n",
    "df_test = pd.read_table('data/test.csv',sep=',',header=0)\n",
    "sample_submission = pd.read_table('data/sample_submission.csv',sep=',',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset contains 159571 examples.\n",
      "The dataframe is composed of 8 columns:\n",
      "id\n",
      "comment_text\n",
      "toxic\n",
      "severe_toxic\n",
      "obscene\n",
      "threat\n",
      "insult\n",
      "identity_hate\n"
     ]
    }
   ],
   "source": [
    "print(\"The training dataset contains\",df_train.shape[0],\"examples.\")\n",
    "print(\"The dataframe is composed of\",df_train.shape[1],\"columns:\")\n",
    "for i in range(len(df_train.columns)): print(df_train.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train = df_train['id']\n",
    "x_train = df_train['comment_text']\n",
    "y_train = df_train.iloc[:, 2:].values\n",
    "\n",
    "id_test = df_test['id']\n",
    "x_test = df_test['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### display some examples of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55964, 91486, 127653]\n",
      "Comment:  And you'll be pleased to know that this article has now come back to life with some decent references. OIC and myself are in the middle of sourcing even more for it, so don't be suprised if it gets further expansion. This is us being a little spiteful towards a user who doesn't have a snowflakes chance in hell of getting the mop.  (talk)\n",
      "Labels:    [1 0 0 0 0 0]\n",
      "\n",
      "\n",
      "Comment:  No it's not. There are still several typos, and the overall writing could be better. ♬♩  (talk)\n",
      "Labels:    [0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "Comment:  \"\n",
      " Speedy deletion of \"\"27 Tricor Ave. New Paltz, NY 12561\"\" \n",
      "\n",
      " A page you created, 27 Tricor Ave. New Paltz, NY 12561, has been tagged for deletion, as it meets one or more of the criteria for speedy deletion; specifically, it is nonsense or gibberish.\n",
      "\n",
      "You are welcome to contribute content which complies with our content policies and any applicable inclusion guidelines. However, please do not simply re-create the page with the same content. You may also wish to read our introduction to editing and guide to writing your first article.\n",
      "\n",
      "Thanks. Ŧħę௹ɛя㎥ \"\n",
      "Labels:    [0 0 0 0 0 0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select randomly some examples\n",
    "examples = random.sample(range(x_train.shape[0]),3)\n",
    "\n",
    "print(examples)\n",
    "for ex in examples:\n",
    "    print('Comment:  {}'.format(x_train[ex]))\n",
    "    print('Labels:    {}'.format(y_train[ex]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tokenizer, which whill be use to vectorize the text of the comments (see: https://keras.io/preprocessing/text/ ).   \n",
    "Then, fit the tokenizer on the training dataset and run it on the training and testing comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "max_num_words = None\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n',\n",
    "                      lower=True, split=' ', char_level=False, oov_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer\n",
    "tokenizer.fit_on_texts(list(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer founds 210337 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index # dictionnary of words whith their indexes (uniquely assigned integers)\n",
    "\n",
    "voc_size = len(word_index)\n",
    "print('The tokenizer founds {} unique tokens.' .format(voc_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokenizer\n",
    "seq_train = tokenizer.texts_to_sequences(x_train)\n",
    "seq_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the training sequences is 68.22156908210138 with a standard deviation of 101.07344657013672.\n",
      "The longest sequence has a size of 1403, and the shortest only 1.\n"
     ]
    }
   ],
   "source": [
    "avg_len = np.mean([len(seq) for seq in seq_train])\n",
    "std_len = np.std([len(seq) for seq in seq_train])\n",
    "longest_len = max(len(seq) for seq in seq_train)\n",
    "shortest_len = min(len(seq) for seq in seq_train)\n",
    "\n",
    "print(\"The average length of the training sequences is {} with a standard deviation of {}.\".format(avg_len,std_len))\n",
    "print(\"The longest sequence has a size of {}, and the shortest only {}.\".format(longest_len,shortest_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length for the sequences is fixed at 371.\n"
     ]
    }
   ],
   "source": [
    "# Fix a maximum length for the sequences\n",
    "max_len = int(avg_len + std_len * 3)\n",
    "print(\"The maximum length for the sequences is fixed at {}.\" .format(maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pads the sequences to the same length : \"max_size\". Sequences shorter than \"max_size\" are padded with \"0\" and the end, and sequences longer are truncated.   \n",
    "(see https://keras.io/preprocessing/sequence/ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data tensor: (159571, 371)\n",
      "Shape of test data tensor: (153164, 371)\n"
     ]
    }
   ],
   "source": [
    "data_train = pad_sequences(seq_train, maxlen=max_len, padding='post', truncating='post', value=0)\n",
    "data_test = pad_sequences(seq_test, maxlen=max_len, padding='post', truncating='post', value=0)\n",
    "\n",
    "print('Shape of training data tensor:', data_train.shape)\n",
    "print('Shape of test data tensor:', data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
